# Churn Analysis
# Set Working Directory
setwd( “C:/users/Priyanka/Documents/R scripts/Assignment/”)
#install.packages(“caret”)
#Load Libraries
library(caret)
library(rpart)
library(c50)
library(rattle)
library(party)
library(partykit)
library(randomForest)
library(RCOR)
library(ggplot2)
library(reshape2)
library(car)
library(corrplot)
library(e1071)
#Load the data
mydata <- read.csv(“Churn.csv”)
#Reencode variables to get into numeric form.
mydata$Churn <- as.integer(mydata$Churn)
mydata$InternationalPlan <- as.integer(Mydata$InternationalPlan)
mydata$VoiceMail <- as.integer(Mydata$ Mydata$VoiceMail)
mydata$Churn[Mydata$Churn==“1”] <- 0
mydata$Churn[Mydata$Churn==“2”] <- 1
mydata$InternationalPlan[Mydata$InternationalPlan==“1”] <- 0
mydata$InternationalPlan[Mydata$InternationalPlan==“2”] <- 1
mydata$VoiceMail[Mydata$VoiceMail==“1”] <- 0
mydata$VoiceMail[Mydata$VoiceMail==“2”] <- 1
# Drop unneeded variables
mydata$State <- Null
mydata$Areacode <- Null
mydata$Phone.Number <- Null
# Handling of missing values.
# This will remove any observation that are missing from the dataset (NULL)
na.omit(mydata)
# Exploratory Data Analysis
# Run a simple a summary to determine if the dataset is complete
summary(mydata)
# This will provide the Standard deviations in the Data.
sapply(mydata, sd)
# Correlation Matrix
cormatrix  <- round(cor(mydata), digits=2)
cormatrix
# Heatmap of correlations using ggplot2
qplot(x=Var1, y=Var2, data=melt(cor(mydata), use=”p”)), fill=value, geom=”tile”) + scale_fill_gradient2(limits-c(-1, 1))
plot.new()
plot(mydata$Churn-mydata$TotDayMin)
title(“Basic Scatterplot”)
# Histogram for TotDayMin
plot.new()
hist(mydata$TotDayMin)
plot.new()
boxplot(mydata$TotDayMin)
title(“Boxplot of TotDayMin”)
# Better graphics from the ggplot library
# ggplot2 of the scatterplot for 1 variable
ggplot(mydata, aes(x=mydata$TotDayMin, y=mydata$TotDayMin)) + geom_point(size
# Scatterplot for 1 variable grouped by Churn
ggplot(mydata, aes(x=mydata$TotDayMin, y=mydata$TotDayMin, colour=mydata$Churn

#Nice looking ggplot2 histogram
ggplot(mydata, aes(x=mydata$TotDayMin)) + geom_histogram(binwidth=1, fill=”white”, colour=”black”)

# This code will split the dataset into a train and test set.
# We will use a 70% training & 30% testing split here. Prob=c(0.7, 0.3)
set.seed(1234)
ind <- sample(2, nrow(mydata), replace=TRUE, prob=c(0.7, 0.3))
trainData <- mydata[ind==1, ]
testData <- mydata[ind==2, ]
# Model 1 – Logistic Regression model
# Select the variables to use based off of the forward selection procedure.
# Lower AIC indicates a better model.
# Forward Elimination
forwardtest <- step(glm(churn=1, data=trainData), direction=”forward”, scope = (Account Length + InternationalPlan + VoiceMail + NumVoiceMail + TotDayMin + TotDayCall+
        TotDayCharge + TotEveMin + TotEveCall + TotEveCharge +  TotNightMin + TotNightCall
        TotNightCharge + TotIntlMin + TotIntlCall + TotIntlCharge + NumCustServCall)
# capture.output(forwardtest.file-“test2b.doc”)
# Here is the logistic regression model based off of the results.
mylogit <- glm(churn ~ TotDayCharge + TotNightCall + NumVoiceMail + TotEveCall + TotDayCall TotIntCharge + TotNightMin + TotEveMin, data = trainData, family = “binomial”)
summary(mylogit)
# capture.output(summary(mylogit).file=”test2a.doc”)
# This section will evaluate the models fit and performance.
# influence plot.
influenceIndexPlot(mylogit, vars-c(“cook”, “hat”), id.n-3)
# Cis using profiled log-likelihood
confint(mylogit)
# Cis using standard errors
confint.default(mylogit)
# Put the coefficients and CI in a format onto a useful scale.
exp(mylogit$coefficients)
exp(confint(mylogit)
# Odds ratios only
exp(coef(mylogit))
# Odd rations and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
# Model 2. Support Vector Machines
# Create a SVM from the tuned parameters
SVMModel <- svm(churn~.,  data = trainData, gamma = 0.1, cost = 1)
print(SVMModel)
summary(SVMModel)
# Model 3.  – Random Forest Model 
RandomForestModel <- randomforest(Churn~., data= trainData, ntree=500, mtree=500, importance = TRUE)
print(RandomForestModel)
importance(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel, type=1, pch=19, col=1, cex=1.0, main=””)
abline(v=45, col=”blue”)
plot.new()
varImpPlot(RandomForestModel, type=2, pch=19, col=1, cex=1.0, main=””)
abline(v=45, col=”blue”)


# Knowledge discovery: Build a decision tree using c5.0 for churn
# The decision variable class must be converted into a factor
# variable in order for the c50 to process correctly
mydata$churn <- as.factor (mydata$churn)
# Run the c50 algorithm for a decision tree.
c50_tree_result <- c5.0(churn~., data= mydata)
# Display the summary 
summary(c50_tree_result)
c5imp(c50_tree_reult, metric=’usage’)
c5imp(c50_tree_reult, metric=’splits’)
# Run the c50 algorithm and show the decision rules.
c50_rule_result <c5.0(churn~. , data=mydata, rules=TRUE)
# display the summary
summary(c50_rule_result)
# Rule 7, 9 and 13
# Evaluate the primary models performance
# this is the function which creates the prediction onto the new dataset.
LogisticModel <- predict(mylogit, testData, type=”response”)
SVMResult <- predict(SVMModel, testData, type=”response”)
RFResult <- predict(RandomForestModel, testData, type= “response”)
# This will create the results as a new column on the dataset 
testData$YHat1 <- predict(mylogit, testData, type=”response”)
testData$YHat2 <- predict(SVMModel, testData, type=”response”)
testData$YHat3 <- predict(RandomForestModel, testData, type= “response”)
# These are threshold parameter setting controls
predict <- function(t) ifelse(LogisticModel > t , 1, 0)
predict2 <- function(t) ifelse(SVMResult > t , 1, 0)
predict3 <- function(t) ifelse(RFResult > t , 1, 0)
confusionMatrix(Predict(0.5), testData$Churn)  #LogisticModel
confusionMatrix(Predict2(0.5), testData$Churn)  #SVM
confusionMatrix(Predict3(0.5), testData$Churn)  #Random Forest
# ROC for unpruned model
pred1 <- prediction(testData$YHat1, testData$Churn) 
pred2 <- prediction(testData$YHat2, testData$Churn)
pred3 <- prediction(testData$YHat3, testData$Churn)
perf <- performance (pred1, “tpr”, “fpr”)
perf2 <- performance (pred2, “tpr”, “fpr”)
perf3 <- performance (pred3, “tpr”, “fpr”)
plot.new()
plot(perf, col = “green”, lwd = 2.5)
plot(perf2, add = TRUE, col = “blue”, lwd = 2.5)
plot(perf3, add = TRUE, col = “orange”, lwd = 2.5)
abline(0, 1, col = “Red”, lwd = 2.5, lty = 2)
title(“ROC Curve”)
legend(0, 8, 0, 4, c(“Logistic”, “SVM”, “RF”), lty = c(1, 1, 1), #gives the legend appropriate symbols (lines) lwd = c(1, 4, 1, 4, 1, 4), col=c(“green”, “blue”, “orange”, “yellow”) # gives the legend lines the correct color and width
#AUC calculation metrics
fit.auc1<- performance(pred1, “auc”)
fit.auc2<- performance(pred2, “auc”)
fit.auc3 <- performance(pred3, “auc”)
fit.auc1 #Logistic regression – AUC 0.69
fit.auc2 # SVM – AUC 0.93
fit.auc3 #Random Forest – AUC 0.94
# Save the model to a file 
save (RandomForestModel, file = “churnmodel.rda”)
# Running a saved model – Automation example
setwd(“C:/users/Priyanka/Documents/R scripts/Assignment/ newBuisness”)
# Import the data from CSV file
mydata <- read.csv(“C:/users/Priyanka/Documents/R scripts/Assignment/ newBuisness”)
# Define the threshold parameter – Data Scientist Provided
threshold <- 0.5
# Load the model
load(“ChurnModel.rda”)
# Run the model on the new mydata dataset
mydata$ChurnPrediction <- predict(RandomForestModel, newdata = mydata, type = “class”)
# Round Prediction
mydata$ChurnPRediction <- ifelse(mydata$ChurnPrediction > threshold, 1, 0)
# Write results into CSV 
write.csv(mydata, file=“C:/users/Priyanka/Documents/R scripts/Assignment/ newBuisness”)
